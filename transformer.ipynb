{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cd297fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8259b56050>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import embed\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.manual_seed(666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c1ca2e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # Read text as one giant string\n",
    "    with open(\"./input.txt\", 'r') as f:\n",
    "        text = f.read()\n",
    "        f.close()\n",
    "    text = text[:int(0.1*len(text))]\n",
    "    text_len = len(text)\n",
    "    print(text_len)\n",
    "\n",
    "    # Get all characters in data, (extra, not required->) rearrange so letters come first\n",
    "    vocab = sorted(list(set(text)))\n",
    "    vocab = vocab[:1] + vocab[13:] + vocab[1:13]\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    # Convert between chars and ints\n",
    "    int2char = {i: vocab[i] for i in range(vocab_size)} # ints -> chars\n",
    "    char2int = {vocab[i]:i for i in range(vocab_size)}   # chars -> ints\n",
    "    text_ints = [char2int[text[i]] for i in range(len(text))]\n",
    "    train = text_ints[:int(0.8*text_len)]\n",
    "    val = text_ints[int(0.8*text_len):int(0.9*text_len)]\n",
    "    test = text_ints[int(0.9*text_len):]\n",
    "    assert len(train)+len(val)+len(test)==text_len\n",
    "    \n",
    "    return train,val,test,vocab_size,int2char\n",
    "\n",
    "    # Check char-int conversion:\n",
    "    # for i in range(10):\n",
    "    #     assert int2char[text_ints[i]]==text[i]\n",
    "\n",
    "    # magic_nums = {}\n",
    "    # magic_nums[\"text_len\"] = text_len\n",
    "    # magic_nums[\"vocab_size\"] = vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b6c41ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chunks(Dataset):\n",
    "    def __init__(self, data, chunk_size):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.chunk_size = chunk_size\n",
    "    def __getitem__(self, indx):\n",
    "        x = self.data[indx:indx+self.chunk_size]\n",
    "        y = self.data[indx+1:indx+self.chunk_size+1]\n",
    "        return torch.tensor(x), torch.tensor(y)\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.chunk_size\n",
    "\n",
    "\n",
    "# Check for off-by-one index issues:  \n",
    "# train_set = Chunks(train[:8], chunk_size=4)\n",
    "# train_loader = DataLoader(train_set, batch_size=1, shuffle=False, drop_last=False)\n",
    "# print(train_set.data)\n",
    "# for x,y in train_loader:\n",
    "#     print(x)\n",
    "#     print(\"    \" + str(y))\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "87570c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import einsum\n",
    "from einops import rearrange, reduce, repeat\n",
    "\n",
    "class Projections(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.Q = nn.Linear(d, d, bias=False)\n",
    "        self.K = nn.Linear(d, d, bias=False)\n",
    "        self.V = nn.Linear(d, d, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        q = self.Q(x)\n",
    "        k = self.K(x)\n",
    "        v = self.V(x)\n",
    "        return q,k,v\n",
    "    \n",
    "\n",
    "class MultiHead(nn.Module):\n",
    "    def __init__(self, d, n_heads):\n",
    "        super().__init__()\n",
    "        self.d = torch.tensor(d)\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "    def forward(self, q, k, v):\n",
    "        # Reshape to create heads dimension.\n",
    "        q = rearrange(q, \"b c (n_heads d_head) -> b c n_heads d_head\", n_heads=self.n_heads)\n",
    "        k = rearrange(k, \"b c (n_heads d_head) -> b c n_heads d_head\", n_heads=self.n_heads)\n",
    "        v = rearrange(v, \"b c (n_heads d_head) -> b n_heads c d_head\", n_heads=self.n_heads)\n",
    "        # Scaled dot product\n",
    "        qk = einsum(\"b Q n d, b K n d -> b n Q K\", q, k) # capital Q,K refer to sequence length axis\n",
    "        qk = qk / torch.sqrt(self.d)\n",
    "        # Future masking every (cq, ck)-shaped square matrix\n",
    "        mask = torch.ones(qk.shape, device='cuda')\n",
    "        mask = torch.tril(mask, diagonal=0)\n",
    "        qk.masked_fill_(mask==0, value=float('-inf'))\n",
    "        # Softmax to get attention weights\n",
    "        attention = F.softmax(qk, dim=-1)\n",
    "        # Apply attention*values\n",
    "        output = einsum(\"b n Q K, b n K d -> b n Q d\", attention, v)\n",
    "        # Reshape to remove heads dimension\n",
    "        output = rearrange(output, \"b n cq d -> b cq (n d)\")\n",
    "        return output\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d, n_heads, mlp_width=6):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.n_heads = n_heads\n",
    "        self.mlp_width = mlp_width\n",
    "        \n",
    "        self.LN1 = nn.LayerNorm(d)\n",
    "        self.PROJ = Projections(d)\n",
    "        self.MHA = MultiHead(d, n_heads)\n",
    "        self.LN2 = nn.LayerNorm(d)\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(d, d*mlp_width),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d*mlp_width, d)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x0):\n",
    "        # Attention\n",
    "        q,k,v = self.PROJ(self.LN1(x0))\n",
    "        x1 = self.MHA(q,k,v)\n",
    "        x1 += x0\n",
    "        # MLP\n",
    "        x2 = self.MLP(self.LN2(x1))\n",
    "        x2 += x1\n",
    "        return x2\n",
    "    \n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, d, vocab_size, chunk_size, n_heads=6, n_blocks=6):\n",
    "        super().__init__()\n",
    "        # Hyperparams\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d = d\n",
    "        self.chunk_size = chunk_size\n",
    "        self.n_heads = n_heads\n",
    "        self.n_blocks = n_blocks\n",
    "        # Initial embeddings\n",
    "        self.E_pos = nn.Embedding(chunk_size, d)\n",
    "        self.E_token = nn.Embedding(vocab_size, d)\n",
    "        # Decoder blocks\n",
    "        self.DBlocks = nn.ModuleList([DecoderBlock(d, n_heads) for i in range(n_blocks)])\n",
    "        # Linear map to logits of distribution over vocab\n",
    "        self.LN_final = nn.LayerNorm(d)\n",
    "        self.logits = nn.Linear(d, vocab_size)\n",
    "    def forward(self, x_ints):\n",
    "        # Embeddings\n",
    "        x_pos = self.E_pos(torch.arange(x_ints.shape[1], device='cuda'))\n",
    "        x_pos = rearrange(x_pos, \"c d -> 1 c d\")\n",
    "        x_token = self.E_token(x_ints)\n",
    "        x1 = x_token + x_pos\n",
    "        # Decoder blocks\n",
    "        for db in self.DBlocks:\n",
    "            x1 = db(x1)\n",
    "        # Linear to logits\n",
    "        x1 = self.LN_final(x1)\n",
    "        x2 = self.logits(x1) \n",
    "        return x2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0b13af",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d526653f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111539\n",
      "LanguageModel(\n",
      "  (E_pos): Embedding(256, 384)\n",
      "  (E_token): Embedding(61, 384)\n",
      "  (DBlocks): ModuleList(\n",
      "    (0): DecoderBlock(\n",
      "      (LN1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (PROJ): Projections(\n",
      "        (Q): Linear(in_features=384, out_features=384, bias=False)\n",
      "        (K): Linear(in_features=384, out_features=384, bias=False)\n",
      "        (V): Linear(in_features=384, out_features=384, bias=False)\n",
      "      )\n",
      "      (MHA): MultiHead()\n",
      "      (LN2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=384, out_features=2304, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=2304, out_features=384, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): DecoderBlock(\n",
      "      (LN1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (PROJ): Projections(\n",
      "        (Q): Linear(in_features=384, out_features=384, bias=False)\n",
      "        (K): Linear(in_features=384, out_features=384, bias=False)\n",
      "        (V): Linear(in_features=384, out_features=384, bias=False)\n",
      "      )\n",
      "      (MHA): MultiHead()\n",
      "      (LN2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=384, out_features=2304, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=2304, out_features=384, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (2): DecoderBlock(\n",
      "      (LN1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (PROJ): Projections(\n",
      "        (Q): Linear(in_features=384, out_features=384, bias=False)\n",
      "        (K): Linear(in_features=384, out_features=384, bias=False)\n",
      "        (V): Linear(in_features=384, out_features=384, bias=False)\n",
      "      )\n",
      "      (MHA): MultiHead()\n",
      "      (LN2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=384, out_features=2304, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=2304, out_features=384, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (3): DecoderBlock(\n",
      "      (LN1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (PROJ): Projections(\n",
      "        (Q): Linear(in_features=384, out_features=384, bias=False)\n",
      "        (K): Linear(in_features=384, out_features=384, bias=False)\n",
      "        (V): Linear(in_features=384, out_features=384, bias=False)\n",
      "      )\n",
      "      (MHA): MultiHead()\n",
      "      (LN2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=384, out_features=2304, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=2304, out_features=384, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (4): DecoderBlock(\n",
      "      (LN1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (PROJ): Projections(\n",
      "        (Q): Linear(in_features=384, out_features=384, bias=False)\n",
      "        (K): Linear(in_features=384, out_features=384, bias=False)\n",
      "        (V): Linear(in_features=384, out_features=384, bias=False)\n",
      "      )\n",
      "      (MHA): MultiHead()\n",
      "      (LN2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=384, out_features=2304, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=2304, out_features=384, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (5): DecoderBlock(\n",
      "      (LN1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (PROJ): Projections(\n",
      "        (Q): Linear(in_features=384, out_features=384, bias=False)\n",
      "        (K): Linear(in_features=384, out_features=384, bias=False)\n",
      "        (V): Linear(in_features=384, out_features=384, bias=False)\n",
      "      )\n",
      "      (MHA): MultiHead()\n",
      "      (LN2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=384, out_features=2304, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=2304, out_features=384, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (LN_final): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (logits): Linear(in_features=384, out_features=61, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 256\n",
    "batch_size = 64\n",
    "d = 384\n",
    "\n",
    "train, val, test, vocab_size, int2char = load_data()\n",
    "train_set = Chunks(train, chunk_size=chunk_size)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "model = LanguageModel(d, vocab_size, chunk_size)\n",
    "print(model)\n",
    "\n",
    "lr = 3e-4\n",
    "epochs = 1\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78afaff6",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1522577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_eval(model, val, batch_size):\n",
    "    val_set = Chunks(val, chunk_size=model.chunk_size)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, drop_last=True)\n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x,y in tqdm(val_loader):\n",
    "            x = x.to('cuda')\n",
    "            y = y.to('cuda')\n",
    "            preds = model(x)\n",
    "            preds = rearrange(preds, \"b c vocab_size -> (b c) vocab_size\")\n",
    "            y = rearrange(y, \"b c -> (b c)\")\n",
    "            loss = F.cross_entropy(preds, y)\n",
    "            val_loss += loss.item()\n",
    "    model.train()\n",
    "    return val_loss / len(val_loader)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "799eec05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1391/1391 [02:22<00:00,  9.76it/s]\n",
      "100%|██████████| 170/170 [00:06<00:00, 25.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0:\n",
      "        Train loss: 1.4098656814596278\n",
      "        Val   loss: 3.188582059916328\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# Move to GPU\n",
    "model = model.to('cuda')\n",
    "model.train()\n",
    "epoch_losses = []\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    for x,y in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x = x.to('cuda')\n",
    "        y = y.to('cuda')\n",
    "        preds = model(x)\n",
    "        preds = rearrange(preds, \"b c vocab_size -> (b c) vocab_size\")\n",
    "        y = rearrange(y, \"b c -> (b c)\")\n",
    "        loss = F.cross_entropy(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss = val_eval(model, val, batch_size)\n",
    "    epoch_losses.append((train_loss, val_loss))\n",
    "    print(f\"Epoch #{epoch}:\")\n",
    "    print(f\"        Train loss: {train_loss}\")\n",
    "    print(f\"        Val   loss: {val_loss}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927bd516",
   "metadata": {},
   "source": [
    "### Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "68cc3d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12160/2416344081.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(logits/temp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "MENENIUS:\n",
      "Has shall straight his lip and abhugh?\n",
      "\n",
      "First Citizen:\n",
      "To lose it forming the to bear abbarbaced!' But,\n",
      "What corrrouse the may disgrace as whiles\n",
      "That flatters in his naturers, you change he fight\n",
      "Wither have but lived more than you necesar\n",
      "He lead the pand what if hor not ine, and who her\n",
      "Their vowsest of their complaninings; and their stong\n",
      "Before Coriolanus he draily is nature, and his blood\n",
      "To the people, who it show'd upon state,\n",
      "He weds mine honour and of angatin\n",
      "He honour in's enreches like about friend\n",
      "Than Now the foolsh of them.\n",
      "\n",
      "COMINIUS:\n",
      "Though there's a lefter\n",
      "The beggarn of the senate, who could\n",
      "But field and city \n",
      "And braile stinctly: if my were statnd to be with the fault,\n",
      "But my daughtines bad with a liess lift them\n",
      "Breal and cortable the people, who ne'er show'd\n",
      "The dust of aughter, even still was like fatte\n",
      "To least the city is proud. Here come begin\n",
      "With here he senaters: and shall his farst,\n",
      "We care a shim friers; fand gener bed,\n",
      "I could be like the flat\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "# Generate text\n",
    "temp = 1.0\n",
    "context = [0]\n",
    "context_char = []\n",
    "with torch.no_grad():\n",
    "    for i in range(1000):\n",
    "        if len(context) > model.chunk_size:\n",
    "            x = torch.tensor(context[-model.chunk_size:])\n",
    "        else:\n",
    "            x = torch.tensor(context)\n",
    "        x = torch.unsqueeze(x, dim=0)\n",
    "        x = x.to('cuda')\n",
    "        logits = model(x)\n",
    "        logits = torch.squeeze(logits[:,-1:,:]) # (B=1, c, d). Take last index along c, remove batch dimension.\n",
    "        probs = F.softmax(logits/temp)\n",
    "        # Sampling: show where probability mass is concentrated,\n",
    "        # take argmax or sample\n",
    "#         print(torch.topk(probs,k=5))\n",
    "#         next_int = int(torch.argmax(probs))\n",
    "        next_int = int(torch.multinomial(probs, num_samples=1))\n",
    "        context.append(next_int)\n",
    "    for i in range(len(context)):\n",
    "        next_char = int2char[context[i]]\n",
    "        context_char.append(next_char)\n",
    "print(''.join(context_char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808ea6bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
