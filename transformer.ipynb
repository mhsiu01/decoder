{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2cd297fc",
      "metadata": {
        "id": "2cd297fc",
        "outputId": "1cd156fe-d8a4-4211-c411-5e32947562f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ea6950e7a50>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import embed\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "torch.manual_seed(666)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c1ca2e71",
      "metadata": {
        "id": "c1ca2e71"
      },
      "outputs": [],
      "source": [
        "def load_data():\n",
        "    !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "    # Read text as one giant string\n",
        "    with open(\"./input.txt\", 'r') as f:\n",
        "        text = f.read()\n",
        "        f.close()\n",
        "    text = text[:int(0.1*len(text))]\n",
        "    text_len = len(text)\n",
        "    print(text_len)\n",
        "\n",
        "    # Get all characters in data, (extra, not required->) rearrange so letters come first\n",
        "    vocab = sorted(list(set(text)))\n",
        "    vocab = vocab[:1] + vocab[13:] + vocab[1:13]\n",
        "    vocab_size = len(vocab)\n",
        "\n",
        "    # Convert between chars and ints\n",
        "    int2char = {i: vocab[i] for i in range(vocab_size)} # ints -> chars\n",
        "    char2int = {vocab[i]:i for i in range(vocab_size)}   # chars -> ints\n",
        "    text_ints = [char2int[text[i]] for i in range(len(text))]\n",
        "    train = text_ints[:int(0.8*text_len)]\n",
        "    val = text_ints[int(0.8*text_len):int(0.9*text_len)]\n",
        "    test = text_ints[int(0.9*text_len):]\n",
        "    assert len(train)+len(val)+len(test)==text_len\n",
        "\n",
        "    return train,val,test,vocab_size,int2char\n",
        "\n",
        "    # Check char-int conversion:\n",
        "    # for i in range(10):\n",
        "    #     assert int2char[text_ints[i]]==text[i]\n",
        "\n",
        "    # magic_nums = {}\n",
        "    # magic_nums[\"text_len\"] = text_len\n",
        "    # magic_nums[\"vocab_size\"] = vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b6c41ad3",
      "metadata": {
        "id": "b6c41ad3"
      },
      "outputs": [],
      "source": [
        "class Chunks(Dataset):\n",
        "    def __init__(self, data, chunk_size):\n",
        "        super().__init__()\n",
        "        self.data = data\n",
        "        self.chunk_size = chunk_size\n",
        "    def __getitem__(self, indx):\n",
        "        x = self.data[indx:indx+self.chunk_size]\n",
        "        y = self.data[indx+1:indx+self.chunk_size+1]\n",
        "        return torch.tensor(x), torch.tensor(y)\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.chunk_size\n",
        "\n",
        "\n",
        "# Check for off-by-one index issues:\n",
        "# train_set = Chunks(train[:8], chunk_size=4)\n",
        "# train_loader = DataLoader(train_set, batch_size=1, shuffle=False, drop_last=False)\n",
        "# print(train_set.data)\n",
        "# for x,y in train_loader:\n",
        "#     print(x)\n",
        "#     print(\"    \" + str(y))\n",
        "#     print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "87570c71",
      "metadata": {
        "id": "87570c71",
        "outputId": "13a5a0af-b14c-4d3b-e410-7566ce826df9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.1\n"
          ]
        }
      ],
      "source": [
        "from torch import einsum\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    !pip install einops\n",
        "from einops import rearrange, reduce, repeat\n",
        "\n",
        "class Projections(nn.Module):\n",
        "    def __init__(self, d):\n",
        "        super().__init__()\n",
        "        self.d = d\n",
        "        self.Q = nn.Linear(d, d, bias=False)\n",
        "        self.K = nn.Linear(d, d, bias=False)\n",
        "        self.V = nn.Linear(d, d, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        q = self.Q(x)\n",
        "        k = self.K(x)\n",
        "        v = self.V(x)\n",
        "        return q,k,v\n",
        "\n",
        "\n",
        "class MultiHead(nn.Module):\n",
        "    def __init__(self, d, n_heads):\n",
        "        super().__init__()\n",
        "        self.d = torch.tensor(d)\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "    def forward(self, q, k, v):\n",
        "        # Reshape to create heads dimension.\n",
        "        q = rearrange(q, \"b c (n_heads d_head) -> b c n_heads d_head\", n_heads=self.n_heads)\n",
        "        k = rearrange(k, \"b c (n_heads d_head) -> b c n_heads d_head\", n_heads=self.n_heads)\n",
        "        v = rearrange(v, \"b c (n_heads d_head) -> b n_heads c d_head\", n_heads=self.n_heads)\n",
        "        # Scaled dot product\n",
        "        qk = einsum(\"b Q n d, b K n d -> b n Q K\", q, k) # capital Q,K refer to sequence length axis\n",
        "        qk = qk / torch.sqrt(self.d)\n",
        "        # Future masking every (cq, ck)-shaped square matrix\n",
        "        mask = torch.ones(qk.shape, device='cuda')\n",
        "        mask = torch.tril(mask, diagonal=0)\n",
        "        qk.masked_fill_(mask==0, value=float('-inf'))\n",
        "        # Softmax to get attention weights\n",
        "        attention = F.softmax(qk, dim=-1)\n",
        "        # Apply attention*values\n",
        "        output = einsum(\"b n Q K, b n K d -> b n Q d\", attention, v)\n",
        "        # Reshape to remove heads dimension\n",
        "        output = rearrange(output, \"b n cq d -> b cq (n d)\")\n",
        "        return output\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, d, n_heads, mlp_width=6):\n",
        "        super().__init__()\n",
        "        self.d = d\n",
        "        self.n_heads = n_heads\n",
        "        self.mlp_width = mlp_width\n",
        "\n",
        "        self.LN1 = nn.LayerNorm(d)\n",
        "        self.PROJ = Projections(d)\n",
        "        self.MHA = MultiHead(d, n_heads)\n",
        "        self.LN2 = nn.LayerNorm(d)\n",
        "        self.MLP = nn.Sequential(\n",
        "            nn.Linear(d, d*mlp_width),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d*mlp_width, d)\n",
        "        )\n",
        "\n",
        "    def forward(self, x0):\n",
        "        # Attention\n",
        "        q,k,v = self.PROJ(self.LN1(x0))\n",
        "        x1 = self.MHA(q,k,v)\n",
        "        x1 += x0\n",
        "        # MLP\n",
        "        x2 = self.MLP(self.LN2(x1))\n",
        "        x2 += x1\n",
        "        return x2\n",
        "\n",
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, d, vocab_size, chunk_size, n_heads=6, n_blocks=6):\n",
        "        super().__init__()\n",
        "        # Hyperparams\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d = d\n",
        "        self.chunk_size = chunk_size\n",
        "        self.n_heads = n_heads\n",
        "        self.n_blocks = n_blocks\n",
        "        # Initial embeddings\n",
        "        self.E_pos = nn.Embedding(chunk_size, d)\n",
        "        self.E_token = nn.Embedding(vocab_size, d)\n",
        "        # Decoder blocks\n",
        "        self.DBlocks = nn.ModuleList([DecoderBlock(d, n_heads) for i in range(n_blocks)])\n",
        "        # Linear map to logits of distribution over vocab\n",
        "        self.LN_final = nn.LayerNorm(d)\n",
        "        self.logits = nn.Linear(d, vocab_size)\n",
        "    def forward(self, x_ints):\n",
        "        # Embeddings\n",
        "        x_pos = self.E_pos(torch.arange(x_ints.shape[1], device='cuda'))\n",
        "        x_pos = rearrange(x_pos, \"c d -> 1 c d\")\n",
        "        x_token = self.E_token(x_ints)\n",
        "        x1 = x_token + x_pos\n",
        "        # Decoder blocks\n",
        "        for db in self.DBlocks:\n",
        "            x1 = db(x1)\n",
        "        # Linear to logits\n",
        "        x1 = self.LN_final(x1)\n",
        "        x2 = self.logits(x1)\n",
        "        return x2\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f0b13af",
      "metadata": {
        "id": "0f0b13af"
      },
      "source": [
        "### Create model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d526653f",
      "metadata": {
        "id": "d526653f",
        "outputId": "97a4b4fb-71c2-4318-ecd9-5fb1f17d2869",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-29 20:33:56--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "\rinput.txt.1           0%[                    ]       0  --.-KB/s               \rinput.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2023-09-29 20:33:56 (18.1 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n",
            "111539\n",
            "LanguageModel(\n",
            "  (E_pos): Embedding(256, 384)\n",
            "  (E_token): Embedding(61, 384)\n",
            "  (DBlocks): ModuleList(\n",
            "    (0-5): 6 x DecoderBlock(\n",
            "      (LN1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "      (PROJ): Projections(\n",
            "        (Q): Linear(in_features=384, out_features=384, bias=False)\n",
            "        (K): Linear(in_features=384, out_features=384, bias=False)\n",
            "        (V): Linear(in_features=384, out_features=384, bias=False)\n",
            "      )\n",
            "      (MHA): MultiHead()\n",
            "      (LN2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "      (MLP): Sequential(\n",
            "        (0): Linear(in_features=384, out_features=2304, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=2304, out_features=384, bias=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (LN_final): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "  (logits): Linear(in_features=384, out_features=61, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "chunk_size = 256\n",
        "batch_size = 256\n",
        "d = 384\n",
        "\n",
        "train, val, test, vocab_size, int2char = load_data()\n",
        "train_set = Chunks(train, chunk_size=chunk_size)\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "model = LanguageModel(d, vocab_size, chunk_size)\n",
        "print(model)\n",
        "\n",
        "lr = 3e-4\n",
        "epochs = 1\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78afaff6",
      "metadata": {
        "id": "78afaff6"
      },
      "source": [
        "### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "1522577d",
      "metadata": {
        "id": "1522577d"
      },
      "outputs": [],
      "source": [
        "def val_eval(model, val, batch_size):\n",
        "    val_set = Chunks(val, chunk_size=model.chunk_size)\n",
        "    val_loader = DataLoader(val_set, batch_size=batch_size, drop_last=True)\n",
        "    val_loss = 0.0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x,y in tqdm(val_loader):\n",
        "            x = x.to('cuda')\n",
        "            y = y.to('cuda')\n",
        "            preds = model(x)\n",
        "            preds = rearrange(preds, \"b c vocab_size -> (b c) vocab_size\")\n",
        "            y = rearrange(y, \"b c -> (b c)\")\n",
        "            loss = F.cross_entropy(preds, y)\n",
        "            val_loss += loss.item()\n",
        "    model.train()\n",
        "    return val_loss / len(val_loader)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "799eec05",
      "metadata": {
        "scrolled": false,
        "id": "799eec05",
        "outputId": "50e11fab-c540-4979-ada8-b1ab1f607a11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/348 [00:04<24:18,  4.20s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-f234c5132152>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrearrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"b c -> (b c)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "# Move to GPU\n",
        "model = model.to('cuda')\n",
        "model.train()\n",
        "epoch_losses = []\n",
        "for epoch in range(epochs):\n",
        "    train_loss = 0.0\n",
        "    for x,y in tqdm(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        x = x.to('cuda')\n",
        "        y = y.to('cuda')\n",
        "        preds = model(x)\n",
        "        preds = rearrange(preds, \"b c vocab_size -> (b c) vocab_size\")\n",
        "        y = rearrange(y, \"b c -> (b c)\")\n",
        "        loss = F.cross_entropy(preds, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    val_loss = val_eval(model, val, batch_size)\n",
        "    epoch_losses.append((train_loss, val_loss))\n",
        "    print(f\"Epoch #{epoch}:\")\n",
        "    print(f\"        Train loss: {train_loss}\")\n",
        "    print(f\"        Val   loss: {val_loss}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "927bd516",
      "metadata": {
        "id": "927bd516"
      },
      "source": [
        "### Generate text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "68cc3d60",
      "metadata": {
        "id": "68cc3d60",
        "outputId": "1c3738f0-3ce5-49a3-cb0d-9391f9e17ad7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-8c813bd2d253>:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  probs = F.softmax(logits/temp)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CORIOLANUS:\n",
            "VOLANUS:\n",
            "IA onod at; bles sat lonss.\n",
            "\n",
            "\n",
            "MICIOLANIUS:\n",
            "Uparvemer ho that tharay mfe what-s I th thim shomy updour; at,\n",
            "Thes Serart gome liche mound tharpodven abend us\n",
            "Musise d thare s mo ssthorn leseas, w\n",
            "Fis.\n",
            "\n",
            "MENENENIUS:Le it Epravend o t is mistholrobust ck,\n",
            "Thinknk:\n",
            "Thralis rse! s s tendofe himathe An tho\n",
            "\n",
            "Thendarole has I's ave se nde muay is on\n",
            "lledsort tanere: we berend t deousi\n",
            "Youft's therup, hashe lith'e me hil k,\n",
            "Thoshon'ds;\n",
            "Yoad t.\n",
            "\n",
            "The d, Co hithey COMelit ut sthan hor mm re?\n",
            "\n",
            "At jod COLANIOin'serseetopenga t ft\n",
            "Firofo,\n",
            "Malenconghepirce Cavarn:\n",
            "Gor: hecoles. tis thangh; p; y, d tharan wof sthico?\n",
            "\n",
            "Sho taie thark, tithy belldr lam st hoe whafofon\n",
            "cild I mzey; d sevanovelak n sther y s:\n",
            "Whoul fois tr s finshe iupl heankin sere acig m cowan d fin buout.\n",
            "\n",
            "S:\n",
            "\n",
            "MENorice bendrk's t myous thevirgour's,? I omp,witst ha S!\n",
            "Houllatay RUS:\n",
            "NUS:\n",
            "We me towemurce.\n",
            "Maser oce nd, gr gheitharshe kend. Malll y heswend huleo, tit whem.\n",
            "\n",
            "\n",
            "Thes Comizenis s nceng'dor, whimshy, t hed n \n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "# Generate text\n",
        "use_argmax = False\n",
        "temp = 1.0\n",
        "context = [0]\n",
        "context_char = []\n",
        "with torch.no_grad():\n",
        "    for i in range(1000):\n",
        "        if len(context) > model.chunk_size:\n",
        "            x = torch.tensor(context[-model.chunk_size:])\n",
        "        else:\n",
        "            x = torch.tensor(context)\n",
        "        x = torch.unsqueeze(x, dim=0)\n",
        "        x = x.to('cuda')\n",
        "        logits = model(x)\n",
        "        logits = torch.squeeze(logits[:,-1:,:]) # (B=1, c, d). Take last index along c, remove batch dimension.\n",
        "        probs = F.softmax(logits/temp)\n",
        "        # Sampling: take argmax or sample from distribution\n",
        "        if use_argmax:\n",
        "            print(torch.topk(probs,k=5))\n",
        "            # Show where probability mass is concentrated\n",
        "            next_int = int(torch.argmax(probs))\n",
        "        else:\n",
        "            next_int = int(torch.multinomial(probs, num_samples=1))\n",
        "        context.append(next_int)\n",
        "    for i in range(len(context)):\n",
        "        next_char = int2char[context[i]]\n",
        "        context_char.append(next_char)\n",
        "print(''.join(context_char))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "808ea6bf",
      "metadata": {
        "id": "808ea6bf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}